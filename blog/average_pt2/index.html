<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>
       On the average pt.2
      
    </title>
    <link rel="stylesheet" href="http://www.cs.princeton.edu/~mhalber//assets/style.css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700" rel="stylesheet">
    <script src="https://use.fontawesome.com/df431d87ca.js"></script>

    <script>
      MathJax = { tex: {tags: 'ams', inlineMath: [['$','$'], ['\\(','\\)']] }};
    </script>    
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  
  <body>

    â€‹<div class="header"><div class="header-content">  
      <h1 class="page_title">
         Maciej's notes
        
        </h1>
      <a href="https://twitter.com/maciejhalber"><i class="fa top_nav fa-twitter fa-lg" aria-hidden="true"></i></a>
      <a href="https://github.com/mhalber"><i class="fa top_nav fa-github fa-lg" aria-hidden="true"></i></a>
      <a href="http://www.cs.princeton.edu/~mhalber//blog/"><i class="fa top_nav fa-pencil fa-lg" aria.z/z-hidden="true"></i></a>
      <a href="http://www.cs.princeton.edu/~mhalber/"><i class="fa top_nav fa-book fa-lg" aria-hidden="true"></i></a>
    </div></div>

<h1>On the average pt.2</h1>
<p>In the previous post we looked at the definition of an average and we showed that the mean is a quantity that minimizes the sum of squared distances. Calculating the mean is extremely easy, as we have a closed form formula to do so.</p>
<p>However, there is no silver bullet. The mean suffers from its sensitivity to outliers. Consider the dataset below - 
we have data that is mostly concentrated around a single point, with few outliers concentrated at another position.</p>
<p><img src="empirical_mean.png#center" alt=""></p>
<p>Ideally, we would like our estimate (calculated mean in the figure above) to be close to the true mean, and somehow ignore the outlier points. As you might imagine, outlier data points are common when dealing with real-world data. As such, people have developed a whole myriad of techniques to deal with such situations. One of these techniques involves the use of &ldquo;robust&rdquo; error function, i.e. functions that are somewhat robust to the outliers.</p>
<p>To understand the intuition behind the robust functions, let us start by looking at the sum of squares loss function we worked through last time (for clarity, and without the loss of generality, we can omit the $\frac{1}{2}$ factor):</p>
<p>$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{\bar{a}}\mathcal{L}(\bar{a}, \mathcal{A}) = \argmin_{\bar{a}}\sum_i^n||\bar{a} - a_i||_2^2 \nonumber
$$</p>
<p>Let&rsquo;s rewrite the above equation with $\epsilon_i = ||\bar{a} - a_i||_2$:</p>
<p>$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{\bar{a}}\mathcal{L}(\bar{a}, \mathcal{A}) = \argmin_{\bar{a}}\sum_i^n\epsilon_i^2 \nonumber
$$</p>
<p>We refer to the $\epsilon_i$ as the residual. By writing our loss function in the above form, it hopefully becomes clear that the squaring the residuals was just a choice that we made, as the function can also be written as:</p>
<p>$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{\bar{a}}\mathcal{L}(\bar{a}, \mathcal{A}) = \argmin_{\bar{a}}\sum_i^n\rho(\epsilon_i) \nonumber \text{,}
$$
with $\rho(x) = x^2$. Question then arises, what happens if we replace the square function with some other function? An example of a robust function is a Pseudo-Huber loss, written as $\rho_{huber}(x, \sigma) = \sigma^2\left(\sqrt{1 + (\frac{x}{\sigma})^2} - 1\right)$. The formula might look a little intimidating, however the plot of Pseudo-Huber function reveals that it is simply a function that approximates quadratic ($x^2$) near the origin and an absolute value linear function everywhere else. The parameter $\sigma$ controls how quickly does this function grows (i.e. slope of the linear part).</p>
<p><img src="functions.png#center" alt=""></p>
<p>To find the minimum of that loss, let us follow the usual recipie of taking the derivative of a function with respect to the parameters and setting it equal to zero:</p>
<p>$$
\begin{align}
\frac{\partial L(\bar{a}, \mathcal{A})}{\partial \bar{x}} &amp;= 0 \nonumber \\\<br>
\frac{\partial \sum_i^n\rho_{huber}(\epsilon_i, \sigma)}{\partial \bar{x}} &amp;= 0 \nonumber \\\<br>
\sum_i^n \frac{\partial \rho_{huber}(\epsilon_i, \sigma)}{\partial \bar{x}} &amp;= 0 \nonumber \\\<br>
\sum_i^n \frac{\partial \rho_{huber}(\epsilon_i, \sigma)}{\partial \epsilon_i} \frac{\partial \epsilon_i}{\partial \bar{x}} &amp;= 0 \nonumber
\end{align}
$$</p>
<p>As we have shown in the part 1:
$$
\frac{\partial \epsilon_i}{\partial \bar{x}} = \frac{\bar{x}-x_i}{\epsilon_i}
$$</p>
<p>What is left to do is to calculate the $\frac{\partial \rho_{huber}(\epsilon_i, \sigma)}{\partial \epsilon_i}$:</p>
<p>$$
\begin{align*}
\frac{\partial \rho_{huber}(\epsilon_i, \sigma)}{\partial \epsilon_i} &amp;= \frac{ \partial\left(\sigma^2\left(\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2} - 1\right)\right)}{\partial \epsilon_i} \\\<br>
&amp;= \frac{ \partial\left(\sigma^2\left(\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}\right)\right)}{\partial \epsilon_i} \\\<br>
&amp;= \sigma^2\frac{ \partial\left(\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}\right)}{\partial \epsilon_i} \\\<br>
&amp;= \sigma^2\frac{ \partial\sqrt{t}}{\partial t}\frac{\partial t}{\partial \epsilon_i} \\\<br>
&amp;= \frac{1}{2}\sigma^2t^{-\frac{1}{2}}\frac{\partial (1 + (\frac{\epsilon_i}{\sigma})^2)}{\partial \epsilon_i} \\\<br>
&amp;= \frac{1}{2}\sigma^2t^{-\frac{1}{2}}\frac{\partial (\frac{\epsilon_i}{\sigma})^2}{\partial \epsilon_i} \\\<br>
&amp;= \frac{1}{2}\sigma^2t^{-\frac{1}{2}}\frac{2\epsilon_i}{\sigma^2} \\\<br>
&amp;= \frac{\epsilon_i}{\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}}
\end{align*}
$$</p>
<p>Putting the two together back into the derivative of the loss function:</p>
<p>$$
\frac{\partial L(\bar{a}, \mathcal{A})}{\partial \bar{x}} = \sum_i^n\frac{\epsilon_i}{\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}}\frac{\bar{x}-x_i}{\epsilon_i} = \sum_i^n\frac{1}{\sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}}(\bar{x}-x_i) = 0
$$</p>
<p>Sadly, unlike when $\rho(x) = x^2$, the above formula still contains $\epsilon_i$. As such we can no longer isolate the value for $\bar{a}$ in a closed form. Instead, to find the robust estimate of the mean, we need to use an iterative approach. In this post we will use a variant of <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">&ldquo;Iteratively Reweighted Least Squares&rdquo; (IRLS)</a> technique. Let us write $w_i = \sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}$, and let us further assume for a moment that the $\epsilon_i$ is not a function of $\bar{x}$. This means that we can keep the term $w_i$ fixed. This allows us to write the formula for the variable we are interested in:</p>
<p>$$
\begin{align}
\sum_i^nw_i(\bar{x}-x_i) &amp;= 0 \nonumber \\\<br>
\sum_i^nw_i\bar{x} - \sum_i^nw_ix_i &amp;= 0 \nonumber \\\<br>
\bar{x}\sum_i^nw_i &amp;= \sum_i^nw_ix_i \nonumber \\\<br>
\bar{x} &amp;= \frac{\sum_i^nw_ix_i}{\sum_i^nw_i}
\end{align}
$$</p>
<p>As you possibly recognize, this is a formula for the weighted average! These weights will allow us to discount the outliers that have been &ldquo;skewing&rdquo; the average estimate. Obviously initially we do not know what are the weights, but we also have a formula for those - $w_i = \sqrt{1 + (\frac{\epsilon_i}{\sigma})^2}$! We can thus design an algorithm to compute both the weights and the estimate iteratively:</p>
<p>$$
\begin{align*}
&amp; \text{set all weights }w_i\text{ to }1 \\\<br>
&amp; \textbf{Repeat }\text{until convergence:} \\\<br>
&amp; \text{&mdash; calculate }x^* = \frac{\sum_i^nw_ix_i}{\sum_i^nw_i} \\\ 
&amp; \text{&mdash; calculate residuals } \epsilon_i = ||x^* - x_i||_2 \\\<br>
&amp; \text{&mdash; calculate weights } w_i = \frac{1}{\epsilon_i}\frac{\partial \rho(\epsilon_i)}{\partial \epsilon_i} 
\end{align*}
$$</p>
<p>Note that a single iteration of the above procedure will simply give us a regular mean, i.e. $x^* = \bar{x}$.
However, as we run the algorithm multiple times, the resulting mean estimate becomes closer to the true mean:
<img src="robust_mean.png#center" alt=""></p>
<p>Additionally, the above recipie is readily adjusted to support any robust function, like Tukey&rsquo;s weights, Geman-McClure, etc. A good overview of different loss functions can be found in a paper by Black and Rangarajan<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, pages 41-48.</p>
<p>TODO: Describe why does it work, how did people come up with these functions?</p>
<p>To finish this post off, note that all the techniques that are shown here generalize to robust linear regression, where the residual takes the form of:
$$
\epsilon_i = y_i - f(x_i),
$$
with linear function $f$. I should most likely write down the maths for this derivation as well, but the general reasoning is very similar.</p>
<p>I also need to point out that all of the information in this post is well known in statistics. If you wish to find out more, I&rsquo;d recommend reading up on M-Estimators in statistics, IRLS in general. Also there is under-appreciated and somewhat now-forgotten piece of work by Michael Black and Rangarajan. Lastly, more recently Jon Barron has published a wonderful paper<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> that looks at generalization of the energy functions we discuss here.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://www.cise.ufl.edu/~anand/pdf/ijcv.pdf">On the Unification of Line Processes, Outlier Rejection, and Robust Statistics with Applications in Early Vision</a>; Michael J. Black and Anand Rangarajan; IJCV 1995 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1701.03077">A General and Adaptive Robust Loss Function</a>; Jon Barron; CVPR 2019 <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</br>



<table class="post_nav_bar">
  <tr class="post_nav_row">
    <td class="post_nav_prev"> 
       
        <a href=http://www.cs.princeton.edu/~mhalber/blog/ogl_gradient/>
        <span class="fa-stack fa-lg">
          <i class="fa fa-circle fa-stack-2x"></i>
          <i class="fa fa-angle-left fa-stack-1x fa-inverse"></i>
        </span>
      </a> 
       
    </td>
    <td class="post_nav_home"> 
      <a href="http://www.cs.princeton.edu/~mhalber//blog/">
        <span class="fa-stack fa-lg">
          <i class="fa fa-circle fa-stack-2x"></i>
          <i class="fa fa-home fa-stack-1x fa-inverse"></i>
        </span>
      </a> 
    </td>
    <td class="post_nav_next"> 
       
        <a href=http://www.cs.princeton.edu/~mhalber/blog/average_pt1/>
          <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-angle-right fa-stack-1x fa-inverse"></i>
          </span>
        </a> 
       
    </td>
  </tr>
</table>

  </body>
</html>