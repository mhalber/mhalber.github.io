<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>
       On the average
      
    </title>
    <link rel="stylesheet" href="https://mhalber.github.io//assets/style.css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700" rel="stylesheet">
    <script src="https://use.fontawesome.com/df431d87ca.js"></script>

    <script>
      MathJax = { tex: {tags: 'ams', inlineMath: [['$','$'], ['\\(','\\)']] }};
    </script>    
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  
  <body>

    â€‹<div class="header"><div class="header-content">  
      <h1 class="page_title">
         Maciej's notes
        
        </h1>
      <a href="https://twitter.com/maciejhalber"><i class="fa top_nav fa-twitter fa-lg" aria-hidden="true"></i></a>
      <a href="https://github.com/mhalber"><i class="fa top_nav fa-github fa-lg" aria-hidden="true"></i></a>
      <a href="https://mhalber.github.io//blog/"><i class="fa top_nav fa-pencil fa-lg" aria.z/z-hidden="true"></i></a>
      <a href="https://mhalber.github.io/"><i class="fa top_nav fa-book fa-lg" aria-hidden="true"></i></a>
    </div></div>

<h1>On the average</h1>
<p>Most people are likely familiar with the concept of an average, as we often encouter it in our everyday lives. Questions like: how much money on average do I spend on groceries each week, how much time on average do I spend in front of computer per day, etc., are very common. The average is very simple to calculate - we take some quantities, add them together, and divide the results by the number of the quantities. In mathematical notation it is:</p>
<p>$$
\begin{eqnarray}
\bar{a} = \frac{1}{n}\sum_{i=0}^na_i\text{,}
\label{eq:avg}
\end{eqnarray}
$$
where $a_i$ is either a scalar value or a vector, that is  $a_i \in \mathbb{R}^n$.</p>
<p>An average, or a mean, is a nice little property that tells us something (albeit not much, see <a href="%22https://en.wikipedia.org/wiki/Anscombe's_quartet%22">Anascome&rsquo;s quartet</a>) about our data. However, one might wonder why is the average defined as in equation $\ref{eq:avg}$? We could punt on it and simply state that the above formula is the definition of the mean, and that&rsquo;s the end of discussion. Hopefully, you agree that it is not a very satisfying answer. In this post we dig a little deeper and derive the formula for the average.</p>
<p>When we think about an average, it is useful to consider it in terms of the properties that the average of some dataset posesses. Let us focus for a moment on two-dimensional case, considering sets of points. Let&rsquo;s assume we have a set $\mathcal{A}={a_1, &hellip;, a_n}$ of $n$ points, where each point is represented by a tuple $a_i = (x,y)$. An average $\bar{a}$ of  $\mathcal{A}$ is a point that minimizes the distance between all elements of $\mathcal{A}$:</p>
<p>$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{\bar{a}}L(\bar{a}, \mathcal{A}) = \argmin_{\bar{a}}\frac{1}{2}\sum_i^n|\bar{a} - a_i|_2^2 \nonumber
$$</p>
<p>The notation of $|| \cdot ||_2$ is the $L_2$ norm, or <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm">Euclidean norm</a>, and it is used to describe the length of a vector. As you probably have noticed, we have also squared the distances $||\bar{a} - a_i||_2$. One nice property is that $f(x)=x^2$ is a convex function, which means that it has a global minimum. Additionally, computing the derivative of $f(x)=x^2$ is quite easy.  We also added a scaling factor of $\frac{1}{2}$ to simplify the formula later on. Importantly, it does not change where the minimum is.</p>
<p>Now, since we are dealing with a convex function, to find $\bar{a}$ we need to take the derivative with respect to the parameters $\bar{a} = {\bar{x}, \bar{y} }$and set each of them to zero:
$$
\begin{align*}
\frac{\partial ( \sum_{i}^{n}|\bar{a}-a_i|_2^2)}{\partial\bar{x}} = 0 \\\<br>
\frac{\partial ( \sum_{i}^{n}|\bar{a}-a_i|_2^2)}{\partial\bar{y}} = 0
\end{align*}
$$
Let us work through the math for one of the parameters, the derivations for the other is analogous (the derivation can also be extended to any dimension).
$$
\begin{align}
\frac{\partial(\frac{1}{2}\sum_i^n|\bar{a}-a_i|_2^2)}{\partial\bar{x}} &amp;= 0 \nonumber \\\<br>
\frac{1}{2}\sum_i^n\frac{\partial(|\bar{a}-a_i|_2^2)}{\partial\bar{x}} &amp;= 0 &amp;&amp; \text{// Sum rule} \nonumber\\\<br>
\frac{1}{2}\sum_i^n\frac{\partial\epsilon_i^2}{\partial\bar{x}} &amp;= 0 &amp;&amp; \text{// } \epsilon_i=||\bar{a}-a_i||_2 \nonumber \\\<br>
\frac{1}{2}\sum_i^n\frac{\partial\epsilon_i^2}{\partial\epsilon_i}\frac{\partial\epsilon_i}{\partial\bar{x}} &amp;= 0 &amp;&amp; \text{// Chain rule} \label{eq:chain}
\end{align}
$$</p>
<p>At this point, by the chain rule, we have two derivatives to work out here, the $\frac{\partial\epsilon_i^2}{\partial\epsilon_i}$ and $\frac{\partial\epsilon_i}{\partial\bar{x}}$. Let us work them out one by one. First one is easy enough:
$$
\frac{\partial\epsilon_i^2}{\partial\epsilon_i} = 2\epsilon_i \nonumber
$$
Moving onto the second part:</p>
<p>$$
\begin{align*}
\frac{\partial{\epsilon_i}}{\partial\bar{x}}&amp;=\frac{\partial|\bar{a}-a_i|_2}{\partial{\bar{x}}} \\\<br>
\frac{\partial{\epsilon_i}}{\partial\bar{x}}&amp;=\frac{\partial(\sqrt{(\bar{x}-x_i)^2+(\bar{y}-y_i)^2})}{\partial{\bar{x}}} &amp;&amp; \text{// from definition of the } L_2\text{ norm} \\\<br>
&amp;=\frac{\partial t^\frac{1}{2}}{\partial\bar{x}} &amp;&amp; \text{// }t=(\bar{x}-x_i)^2+(\bar{y}-y_i)^2 \text{ and } t^{\frac{1}{2}} = \sqrt{t} \\\<br>
&amp;=\frac{\partial t^\frac{1}{2}}{\partial t}\frac{\partial t}{\partial\bar{x}} &amp;&amp; \text{// Chain rule}\\\<br>
&amp;=\frac{1}{2}t^{-\frac{1}{2}}\left( \frac{\partial (\bar{x}-x_i)^2}{\partial\bar{x}} + \frac{\partial (\bar{y}-y_i)^2}{\partial\bar{x}} \right) &amp;&amp; \text{// Power rule and Sum rule} \\\<br>
&amp;=\frac{1}{2\epsilon_i}\frac{\partial{\bar{x}^2-2\bar{x}x_i+x_i^2}}{\partial{\bar{x}}} &amp;&amp; \text{// } t^{-\frac{1}{2}} = \frac{1}{\sqrt{(\bar{x}-x_i)^2+(\bar{y}-y_i)^2}} = \frac{1}{\epsilon_i} \\\<br>
&amp;=\frac{1}{2\epsilon_i}(2\bar{x}-2x_i) \\\<br>
&amp;=\frac{1}{2\epsilon_i}2(\bar{x}-x_i) \\\<br>
&amp;=\frac{\bar{x}-x_i}{\epsilon_i}
\end{align*}
$$</p>
<p>That was a bit more work, but hopefully you can see that we just went through some basic transformations. Note also that we are for a moment assuming that $\epsilon_i \neq 0$. If we plug both expressions back into the equation $\ref{eq:chain}$. we can find the formula for $\bar{x}$:
$$
\begin{align*}
\frac{1}{2}\sum_i^n\frac{\partial\epsilon_i^2}{\partial\epsilon_i}\frac{\partial\epsilon_i}{\partial\bar{x}} &amp;= 0 \\\<br>
\frac{1}{2}\sum_i^n2\epsilon_i\frac{\bar{x}-x_i}{\epsilon_i} &amp;= 0\\\<br>
\sum_i^n(\bar{x}-x_i) &amp;= 0 \\\<br>
n\bar{x} -\sum_i^nx_i &amp;= 0 \\\<br>
\bar{x} &amp;= \frac{1}{n}\sum_i^nx_i
\end{align*}
$$</p>
<p>We have arrived at expression for the first coordinate of $\bar{a}$. As mentioned before, similar derivation can be made for $\bar{y}$. Given that the operations like addition and scalar multiplication are defined for both scalars and vectors, we have shown that the average of points in the set $\mathcal{A}$ is equal to the minimizer of the sum of squared distances:
$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{\bar{a}}L(\bar{a}, \mathcal{A}) = \argmin_{\bar{a}}\frac{1}{2}\sum_i^n||\bar{a} - a_i||_2^2 = \frac{1}{n}\sum_i^na_i
$$
To finish off, and to add at least one figure to this post, below we show the average of two dimensional point set:</p>
<p><img src="mean.png#center" alt="Test"></p>
<p>Hopefully this post can be useful to someone. Questions? Comments? Found errors? Feel free to hit me up on twitter <a href="https://twitter.com/maciejhalber">@maciejhalber</a>!</p>
<p>Credits: The visualization above is <strong>highly</strong> inspired by the excellent visualizations in the companion video of the paper <a href="https://www.youtube.com/watch?v=BmNKbnF69eY">&ldquo;A General and Adaptive Robust Loss Function&rdquo;</a> by Jonathan T. Barron.</p>

</br>



<table class="post_nav_bar">
  <tr class="post_nav_row">
    <td class="post_nav_prev"> 
       
        <a href=https://mhalber.github.io/blog/ogl_gradient/>
        <span class="fa-stack fa-lg">
          <i class="fa fa-circle fa-stack-2x"></i>
          <i class="fa fa-angle-left fa-stack-1x fa-inverse"></i>
        </span>
      </a> 
       
    </td>
    <td class="post_nav_home"> 
      <a href="https://mhalber.github.io//blog/">
        <span class="fa-stack fa-lg">
          <i class="fa fa-circle fa-stack-2x"></i>
          <i class="fa fa-home fa-stack-1x fa-inverse"></i>
        </span>
      </a> 
    </td>
    <td class="post_nav_next"> 
       
    </td>
  </tr>
</table>

  </body>
</html>